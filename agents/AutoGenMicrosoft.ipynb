{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36d8a72",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee07842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b360ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='Hello! How can I help you today?' usage=RequestUsage(prompt_tokens=39, completion_tokens=10) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "#%pip install autogen-ext[openai]\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import ModelInfo, UserMessage\n",
    "model_client = OpenAIChatCompletionClient(model=\"xiaomi/mimo-v2-flash:free\", base_url=\"https://openrouter.ai/api/v1\",\n",
    "                                          model_info=ModelInfo(vision=False, function_calling=True, json_output=False, family=\"unknown\", structured_output=False))\n",
    "#this model_info is required for OpenRouter models only, for OpenAI there is no need to pass the model_info\n",
    "\n",
    "result = await model_client.create([UserMessage(content=\"hello World\", source=\"user\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800395b",
   "metadata": {},
   "source": [
    "## Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e6cfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install autogen-agentchat\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "message = TextMessage(content=\"Hello, do you have any information from the tool\", source=\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f133e",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8ef1ec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My agent says I'm not getting peanuts for this gig, so I guess I'm playing for my health.  And let me tell you, my health isn't what it used to be!\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name=\"first_agent\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a stand up comedian.\",\n",
    "    model_client_stream=True)\n",
    "\n",
    "response = await agent.on_messages([message], cancellation_token=CancellationToken())\n",
    "print(response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87582876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trending_topics() -> list:\n",
    "    \"\"\" Information required by the agent \"\"\"\n",
    "    return [\"husband wife\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29a03863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see the trending topic is **\"husband wife\"**. Would you like me to generate some content ideas related to this theme?\n"
     ]
    }
   ],
   "source": [
    "## Agent with tools\n",
    "agent_withTools = AssistantAgent(\n",
    "    name=\"agent_with_tools\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Print whatever the tool returns to you.\",\n",
    "    tools = [get_trending_topics],\n",
    "    model_client_stream=True,\n",
    "    reflect_on_tool_use=True)\n",
    "\n",
    "response = await agent_withTools.on_messages([message], cancellation_token=CancellationToken())\n",
    "print(response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "383aeee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information from the tool yet. To get the trending topics, I need to make a call to the `get_trending_topics` function. Let me do that for you.\n",
      "[FunctionCall(id='call_37a2bb7b6cdb4cb29eec2280', arguments='{}', name='get_trending_topics')]\n",
      "[FunctionExecutionResult(content=\"['husband wife']\", name='get_trending_topics', call_id='call_37a2bb7b6cdb4cb29eec2280', is_error=False)]\n"
     ]
    }
   ],
   "source": [
    "for inner_message in response.inner_messages:\n",
    "    print(inner_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146719a",
   "metadata": {},
   "source": [
    "## Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14a7fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for first_agent_64c4b301-25ad-472b-b525-fab1101323e9/64c4b301-25ad-472b-b525-fab1101323e9\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1093, in _call_llm\n",
      "    async for chunk in model_client.create_stream(\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 908, in create_stream\n",
      "    async for chunk in chunks:\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 1099, in _create_stream_chunks\n",
      "    stream = await stream_future\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1767398400000'}, 'provider_name': None}}, 'user_id': 'user_37WulDpYYUHkMI0NNaoEX94OKj2'}\n",
      "Error processing publish message for agent_with_tools_64c4b301-25ad-472b-b525-fab1101323e9/64c4b301-25ad-472b-b525-fab1101323e9\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1767398400000'}, 'provider_name': None}}, 'user_id': 'user_37WulDpYYUHkMI0NNaoEX94OKj2'}\nTraceback:\nTraceback (most recent call last):\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1093, in _call_llm\n    async for chunk in model_client.create_stream(\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 908, in create_stream\n    async for chunk in chunks:\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 1099, in _create_stream_chunks\n    stream = await stream_future\n             ^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py\", line 1594, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1767398400000'}, 'provider_name': None}}, 'user_id': 'user_37WulDpYYUHkMI0NNaoEX94OKj2'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m text_termination  = TextMentionTermination(\u001b[33m\"\u001b[39m\u001b[33mStop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m team = RoundRobinGroupChat([agent, agent_withTools], termination_condition=text_termination, max_turns=\u001b[32m20\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m team.run(task=\u001b[33m\"\u001b[39m\u001b[33mgo\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m response.messages:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:340\u001b[39m, in \u001b[36mBaseGroupChat.run\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run the team and return the result. The base implementation uses\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[33;03m:meth:`run_stream` to run the team and then returns the final result.\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[33;03mOnce the team is stopped, the termination condition is reset.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m result: TaskResult | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_stream(\n\u001b[32m    341\u001b[39m     task=task,\n\u001b[32m    342\u001b[39m     cancellation_token=cancellation_token,\n\u001b[32m    343\u001b[39m     output_task_messages=output_task_messages,\n\u001b[32m    344\u001b[39m ):\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    346\u001b[39m         result = message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:554\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    555\u001b[39m     stop_reason = message.message.content\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1767398400000'}, 'provider_name': None}}, 'user_id': 'user_37WulDpYYUHkMI0NNaoEX94OKj2'}\nTraceback:\nTraceback (most recent call last):\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1093, in _call_llm\n    async for chunk in model_client.create_stream(\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 908, in create_stream\n    async for chunk in chunks:\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 1099, in _create_stream_chunks\n    stream = await stream_future\n             ^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\vishi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py\", line 1594, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1767398400000'}, 'provider_name': None}}, 'user_id': 'user_37WulDpYYUHkMI0NNaoEX94OKj2'}\n"
     ]
    }
   ],
   "source": [
    "## Teams\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "text_termination  = TextMentionTermination(\"Stop\")\n",
    "team = RoundRobinGroupChat([agent, agent_withTools], termination_condition=text_termination, max_turns=20)\n",
    "\n",
    "response = await team.run(task=\"go\")\n",
    "\n",
    "for message in response.messages:\n",
    "    print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
